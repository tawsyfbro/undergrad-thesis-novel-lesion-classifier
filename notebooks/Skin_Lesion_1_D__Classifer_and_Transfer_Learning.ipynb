{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T14:08:11.545356Z",
     "iopub.status.busy": "2024-09-12T14:08:11.544938Z",
     "iopub.status.idle": "2024-09-12T14:08:32.965097Z",
     "shell.execute_reply": "2024-09-12T14:08:32.964228Z",
     "shell.execute_reply.started": "2024-09-12T14:08:11.545316Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "dataset_dir = '/kaggle/input/isic-separated-2019/separated_ISIC_2019'  # Update this path to your dataset directory\n",
    "\n",
    "# Parameters\n",
    "img_height, img_width = 96, 96\n",
    "num_classes = 8  # Number of classes in the dataset\n",
    "\n",
    "# Function to load images and labels\n",
    "def load_data(dataset_dir, img_height, img_width):\n",
    "    images = []\n",
    "    labels = []\n",
    "    count = 0\n",
    "    \n",
    "    for disease in os.listdir(dataset_dir):\n",
    "        disease_dir = os.path.join(dataset_dir, disease)\n",
    "        if os.path.isdir(disease_dir):\n",
    "            count = 0\n",
    "            for img_name in os.listdir(disease_dir):\n",
    "                img_path = os.path.join(disease_dir, img_name)\n",
    "                img = cv2.imread(img_path)\n",
    "                img = cv2.resize(img, (img_width, img_height))\n",
    "                #img = img.astype('float32') / 255.0\n",
    "                img_array = np.array(img) / 255.0\n",
    "                images.append(img_array)\n",
    "                labels.append(disease)\n",
    "                count+=1 \n",
    "                \n",
    "                if count >= max_images_per_class:\n",
    "                    break\n",
    "                \n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Load the dataset\n",
    "images, labels = load_data(dataset_dir, img_height, img_width)\n",
    "\n",
    "labels.shape\n",
    "\n",
    "unique_values, counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "print(\"Unique values:\", unique_values)\n",
    "print(\"Counts:\", counts)\n",
    "\n",
    "label_binarizer = LabelBinarizer()\n",
    "labels = label_binarizer.fit_transform(labels)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=26, stratify=labels)\n",
    "\n",
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2\n",
    ")\n",
    "\n",
    "\n",
    "cnn_model = Sequential([\n",
    "    # First Conv2D layer\n",
    "    Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(96, 96, 3)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(3, 3)),  # Reduces spatial dimensions to 32x32\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    # Second Conv2D layer\n",
    "    Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "    Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    # Third Conv2D layer\n",
    "    Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
    "    Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    # Flatten layer\n",
    "    Flatten(),\n",
    "    \n",
    "    # Dense layers\n",
    "    Dense(1024, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    # Output layer\n",
    "    Dense(8, activation='softmax')  # 7 classes for classification\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "# Fit the model using data augmentation\n",
    "cnn_model.fit(datagen.flow(X_train, y_train, batch_size=batch_size),\n",
    "              epochs=epochs,\n",
    "              validation_data=(X_test, y_test))\n",
    "\n",
    "\n",
    "# Evaluate the CNN model\n",
    "cnn_loss, cnn_accuracy = cnn_model.evaluate(X_test, y_test, verbose=2)\n",
    "\n",
    "# Predict class probabilities\n",
    "y_pred_probs = cnn_model.predict(X_test)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Check if y_test is in one-hot format and convert to class labels if necessary\n",
    "if y_test.ndim > 1 and y_test.shape[1] > 1:  # Assuming it's one-hot encoded\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Calculate precision, recall, and f1-score for CNN\n",
    "cnn_precision = precision_score(y_test, y_pred, average='weighted')\n",
    "cnn_recall = recall_score(y_test, y_pred, average='weighted')\n",
    "cnn_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Print the classification report\n",
    "print(\"\\nCNN - Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<h1> FEATURE EXTRACTION of 25,331 IMAGES <h1>\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import EfficientNetB7\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "\n",
    "# Set the dataset path\n",
    "dataset_dir = '/kaggle/input/isic-separated-2019/separated_ISIC_2019'  # Update this path to your dataset\n",
    "\n",
    "# Parameters\n",
    "img_height, img_width = 224, 224  # EfficientNetB7 input size\n",
    "batch_size = 32  # Batch size for efficient memory usage\n",
    "num_classes = 8  # Number of categories in your dataset\n",
    "\n",
    "# Load the EfficientNetB7 model pre-trained on ImageNet\n",
    "base_model = EfficientNetB7(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
    "\n",
    "# Add custom layers for feature extraction\n",
    "x = GlobalAveragePooling2D()(base_model.output)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "feature_extractor = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# Define the data generator for loading images in batches and apply augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    preprocessing_function=tf.keras.applications.efficientnet.preprocess_input  # EfficientNetB7 preprocessing\n",
    ")\n",
    "\n",
    "# Use flow_from_directory to load images and labels from the dataset directory in batches\n",
    "image_generator = datagen.flow_from_directory(\n",
    "    dataset_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',  # Generates one-hot encoded labels\n",
    "    shuffle=False  # No shuffling to ensure images and labels are in order\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<h3> EXTRACTION USING EFFICIENT NET B7 </h3>\n",
    "\n",
    "# Initialize empty lists to store features and labels\n",
    "extracted_features = []\n",
    "extracted_labels = []\n",
    "\n",
    "# Process the images and extract features in batches\n",
    "for batch_images, batch_labels in image_generator:\n",
    "    # Extract features for the batch of images\n",
    "    features_batch = feature_extractor.predict(batch_images)\n",
    "    \n",
    "    # Append the extracted features and corresponding labels\n",
    "    extracted_features.append(features_batch)\n",
    "    extracted_labels.append(batch_labels)\n",
    "\n",
    "    # To avoid infinite loop, break when done\n",
    "    if image_generator.batch_index == image_generator.n // batch_size:\n",
    "        break\n",
    "\n",
    "# Convert the lists of features and labels into numpy arrays\n",
    "extracted_features = np.vstack(extracted_features)\n",
    "extracted_labels = np.vstack(extracted_labels)\n",
    "\n",
    "# Save the extracted features and labels to files for later use\n",
    "np.save('extracted_features_full_isic_2019_EFFB7.npy', extracted_features)\n",
    "np.save('extracted_labels_full_isic_2019_EFFB7.npy', extracted_labels)\n",
    "\n",
    "extracted_features.shape\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming the 'extracted_features.npy' and 'extracted_labels.npy' files are already saved\n",
    "# Load the features and labels (skip this if already in memory)\n",
    "# extracted_features = np.load('extracted_features.npy')\n",
    "# extracted_labels = np.load('extracted_labels.npy')\n",
    "\n",
    "# Step 1: Preprocess the labels (Convert one-hot encoded labels back to integers)\n",
    "# If extracted_labels are one-hot encoded, we use argmax to convert back to categorical labels\n",
    "labels = np.argmax(extracted_labels, axis=1)\n",
    "\n",
    "# Step 2: Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(extracted_features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Initialize and train the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Make predictions on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Step 5: Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Print a classification report for detailed performance metrics\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "np.unique(labels, return_counts= True)\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# SMOTE requires a 2D array for features and a 1D array for labels\n",
    "X_resampled, y_resampled = smote.fit_resample(extracted_features, labels)\n",
    "\n",
    "# Step 3: Split the resampled data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Initialize and train the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model on the balanced dataset\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Make predictions on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Step 6: Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy after SMOTE: {accuracy:.4f}\")\n",
    "\n",
    "# Print a classification report for detailed performance metrics\n",
    "print(\"Classification Report after SMOTE:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, BatchNormalization, Dense,Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Conv1D(32, kernel_size=3, activation='relu', input_shape=(1024, 1)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Conv1D(64, kernel_size=3, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),  # Add batch normalization layer\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),  # Add batch normalization layer\n",
    "    Dropout(0.5),\n",
    "    Dense(8, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "history = model.fit(X_train_reshaped, y_train, batch_size=32, epochs=80, validation_split=0.2, verbose=1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred_probs = model.predict(X_test_reshaped)\n",
    "y_pred = np.argmax(y_pred_probs, axis=-1)  # Get the predicted class labels\n",
    "\n",
    "# Generate classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "<h2> END OF EFFICIENTNETB7 </h2>\n",
    "\n",
    "<h2> Extraction using EfficientNetB0 </h2>\n",
    "\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "\n",
    "# Parameters\n",
    "img_height, img_width = 224, 224  # EfficientNetB7 input size\n",
    "batch_size = 32  # Batch size for efficient memory usage\n",
    "num_classes = 8  # Number of categories in your dataset\n",
    "\n",
    "# Load the EfficientNetB7 model pre-trained on ImageNet\n",
    "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
    "\n",
    "# Add custom layers for feature extraction\n",
    "x = GlobalAveragePooling2D()(base_model.output)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "feature_extractor = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# Initialize empty lists to store features and labels\n",
    "extracted_features = []\n",
    "extracted_labels = []\n",
    "\n",
    "# Process the images and extract features in batches\n",
    "for batch_images, batch_labels in image_generator:\n",
    "    # Extract features for the batch of images\n",
    "    features_batch = feature_extractor.predict(batch_images)\n",
    "    \n",
    "    # Append the extracted features and corresponding labels\n",
    "    extracted_features.append(features_batch)\n",
    "    extracted_labels.append(batch_labels)\n",
    "\n",
    "    # To avoid infinite loop, break when done\n",
    "    if image_generator.batch_index == image_generator.n // batch_size:\n",
    "        break\n",
    "\n",
    "# Convert the lists of features and labels into numpy arrays\n",
    "extracted_features = np.vstack(extracted_features)\n",
    "extracted_labels = np.vstack(extracted_labels)\n",
    "\n",
    "# Save the extracted features and labels to files for later use\n",
    "np.save('extracted_features_full_isic_2019_EFFB0.npy', extracted_features)\n",
    "np.save('extracted_labels_full_isic_2019_EFFB0.npy', extracted_labels)\n",
    "\n",
    "extracted_features.shape\n",
    "\n",
    "labels = np.argmax(extracted_labels, axis=1)\n",
    "\n",
    "np.unique(labels, return_counts = True)\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# SMOTE requires a 2D array for features and a 1D array for labels\n",
    "X_resampled, y_resampled = smote.fit_resample(extracted_features, labels)\n",
    "\n",
    "# Step 3: Split the resampled data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Initialize and train the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model on the balanced dataset\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Make predictions on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Step 6: Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy after SMOTE: {accuracy:.4f}\")\n",
    "\n",
    "# Print a classification report for detailed performance metrics\n",
    "print(\"Classification Report after SMOTE:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, BatchNormalization, Dense,Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Conv1D(32, kernel_size=3, activation='relu', input_shape=(1024, 1)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Conv1D(64, kernel_size=3, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),  # Add batch normalization layer\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),  # Add batch normalization layer\n",
    "    Dropout(0.5),\n",
    "    Dense(8, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "history = model.fit(X_train_reshaped, y_train, batch_size=32, epochs=80, validation_split=0.2, verbose=1)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred_probs = model.predict(X_test_reshaped)\n",
    "y_pred = np.argmax(y_pred_probs, axis=-1)  # Get the predicted class labels\n",
    "\n",
    "# Generate classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "<h2> END OF EFFICIENTNETB0 </h2>\n",
    "\n",
    "<h3> Extraction using ResNet50\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "\n",
    "# Set the dataset path\n",
    "dataset_dir = '/kaggle/input/isic-separated-2019/separated_ISIC_2019'  # Update this path to your dataset\n",
    "\n",
    "# Parameters\n",
    "img_height, img_width = 224, 224  # ResNet50 default input size\n",
    "batch_size = 32  # Batch size for efficient memory usage\n",
    "num_classes = 8  # Number of categories in your dataset\n",
    "\n",
    "# Load the ResNet50 model pre-trained on ImageNet\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
    "\n",
    "# Add custom layers for feature extraction\n",
    "x = GlobalAveragePooling2D()(base_model.output)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "feature_extractor = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# Define the data generator for loading images in batches and apply augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    preprocessing_function=tf.keras.applications.resnet50.preprocess_input  # ResNet50 preprocessing\n",
    ")\n",
    "\n",
    "# Use flow_from_directory to load images and labels from the dataset directory in batches\n",
    "image_generator = datagen.flow_from_directory(\n",
    "    dataset_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',  # Generates one-hot encoded labels\n",
    "    shuffle=False  # No shuffling to ensure images and labels are in order\n",
    ")\n",
    "\n",
    "# Initialize empty lists to store features and labels\n",
    "extracted_features = []\n",
    "extracted_labels = []\n",
    "\n",
    "# Process the images and extract features in batches\n",
    "for batch_images, batch_labels in image_generator:\n",
    "    # Extract features for the batch of images\n",
    "    features_batch = feature_extractor.predict(batch_images)\n",
    "    \n",
    "    # Append the extracted features and corresponding labels\n",
    "    extracted_features.append(features_batch)\n",
    "    extracted_labels.append(batch_labels)\n",
    "\n",
    "    # To avoid infinite loop, break when done\n",
    "    if image_generator.batch_index == image_generator.n // batch_size:\n",
    "        break\n",
    "\n",
    "# Convert the lists of features and labels into numpy arrays\n",
    "extracted_features = np.vstack(extracted_features)\n",
    "extracted_labels = np.vstack(extracted_labels)\n",
    "\n",
    "# Save the extracted features and labels to files for later use\n",
    "np.save('extracted_features_full_isic_2019_ResNet50.npy', extracted_features)\n",
    "np.save('extracted_labels_full_isic_2019_ResNet50.npy', extracted_labels)\n",
    "\n",
    "labels = np.argmax(extracted_labels, axis=1)\n",
    "\n",
    "extracted_features.shape\n",
    "\n",
    "np.unique(labels, return_counts = True)\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# SMOTE requires a 2D array for features and a 1D array for labels\n",
    "X_resampled, y_resampled = smote.fit_resample(extracted_features, labels)\n",
    "\n",
    "# Step 3: Split the resampled data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Initialize and train the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model on the balanced dataset\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Make predictions on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Step 6: Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy after SMOTE: {accuracy:.4f}\")\n",
    "\n",
    "# Print a classification report for detailed performance metrics\n",
    "print(\"Classification Report after SMOTE:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, BatchNormalization, Dense,Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Conv1D(32, kernel_size=3, activation='relu', input_shape=(1024, 1)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Conv1D(64, kernel_size=3, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),  # Add batch normalization layer\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),  # Add batch normalization layer\n",
    "    Dropout(0.5),\n",
    "    Dense(8, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "history = model.fit(X_train_reshaped, y_train, batch_size=32, epochs=80, validation_split=0.2, verbose=1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred_probs = model.predict(X_test_reshaped)\n",
    "y_pred = np.argmax(y_pred_probs, axis=-1)  # Get the predicted class labels\n",
    "\n",
    "# Generate classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "<h3>End of ResNet50 </h3>\n",
    "\n",
    "<h2> Extraction using InceptionNet </h2>\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "\n",
    "# Set the dataset path\n",
    "dataset_dir = '/kaggle/input/isic-separated-2019/separated_ISIC_2019'  # Update this path to your dataset\n",
    "\n",
    "# Parameters\n",
    "img_height, img_width = 299, 299  # InceptionV3 default input size\n",
    "batch_size = 32  # Batch size for efficient memory usage\n",
    "num_classes = 8  # Number of categories in your dataset\n",
    "\n",
    "# Load the InceptionV3 model pre-trained on ImageNet\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
    "\n",
    "# Add custom layers for feature extraction\n",
    "x = GlobalAveragePooling2D()(base_model.output)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "feature_extractor = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# Define the data generator for loading images in batches and apply augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    preprocessing_function=tf.keras.applications.inception_v3.preprocess_input  # InceptionV3 preprocessing\n",
    ")\n",
    "\n",
    "# Use flow_from_directory to load images and labels from the dataset directory in batches\n",
    "image_generator = datagen.flow_from_directory(\n",
    "    dataset_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',  # Generates one-hot encoded labels\n",
    "    shuffle=False  # No shuffling to ensure images and labels are in order\n",
    ")\n",
    "\n",
    "# Initialize empty lists to store features and labels\n",
    "extracted_features = []\n",
    "extracted_labels = []\n",
    "\n",
    "# Process the images and extract features in batches\n",
    "for batch_images, batch_labels in image_generator:\n",
    "    # Extract features for the batch of images\n",
    "    features_batch = feature_extractor.predict(batch_images)\n",
    "    \n",
    "    # Append the extracted features and corresponding labels\n",
    "    extracted_features.append(features_batch)\n",
    "    extracted_labels.append(batch_labels)\n",
    "\n",
    "    # To avoid infinite loop, break when done\n",
    "    if image_generator.batch_index == image_generator.n // batch_size:\n",
    "        break\n",
    "\n",
    "# Convert the lists of features and labels into numpy arrays\n",
    "extracted_features = np.vstack(extracted_features)\n",
    "extracted_labels = np.vstack(extracted_labels)\n",
    "\n",
    "# Save the extracted features and labels to files for later use\n",
    "np.save('extracted_features_full_isic_2019_InceptionNet.npy', extracted_features)\n",
    "np.save('extracted_labels_full_isic_2019_InceptionNet.npy', extracted_labels)\n",
    "\n",
    "labels = np.argmax(extracted_labels, axis=1)\n",
    "\n",
    "extracted_features.shape\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# SMOTE requires a 2D array for features and a 1D array for labels\n",
    "X_resampled, y_resampled = smote.fit_resample(extracted_features, labels)\n",
    "\n",
    "# Step 3: Split the resampled data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Initialize and train the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model on the balanced dataset\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Make predictions on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Step 6: Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy after SMOTE: {accuracy:.4f}\")\n",
    "\n",
    "# Print a classification report for detailed performance metrics\n",
    "print(\"Classification Report after SMOTE:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, BatchNormalization, Dense,Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Conv1D(32, kernel_size=3, activation='relu', input_shape=(1024, 1)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Conv1D(64, kernel_size=3, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),  # Add batch normalization layer\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),  # Add batch normalization layer\n",
    "    Dropout(0.5),\n",
    "    Dense(8, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "history = model.fit(X_train_reshaped, y_train, batch_size=32, epochs=80, validation_split=0.2, verbose=1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred_probs = model.predict(X_test_reshaped)\n",
    "y_pred = np.argmax(y_pred_probs, axis=-1)  # Get the predicted class labels\n",
    "\n",
    "# Generate classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5660557,
     "sourceId": 9340477,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
